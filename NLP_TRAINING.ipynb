{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP_TRAINING.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MWCU0wbkMVDj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagunkayastha/ILab_Tutorials/blob/master/NLP_TRAINING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG1SPYXHtybp",
        "colab_type": "text"
      },
      "source": [
        "#Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHNFmpOTt8mW",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUTg3jiqoj4T",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### ***Why Preprocessing?***\n",
        "\n",
        "Garbage in garbage out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCF-kvK8l3e_",
        "colab_type": "code",
        "outputId": "d83aaae5-9225-4dd6-d0cc-59b73c1fe1fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUu-NuGzon1o",
        "colab_type": "code",
        "outputId": "4a933f52-5115-4059-caf0-284a5efd9a4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = 'This is introduction to NLP. It is likely to be useful, to people since Machine learning is the new electricity'\n",
        "print(text)\n",
        "\n",
        "# fd = nltk.FreqDist(text.split())\n",
        "# fd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is introduction to NLP. It is likely to be useful, to people since Machine learning is the new electricity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoXiYcCFufbo",
        "colab_type": "text"
      },
      "source": [
        "Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAyczM0Rpp9L",
        "colab_type": "code",
        "outputId": "594d0970-c058-4ed5-ae9f-7e92025bb31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# stop words\n",
        "stop_words = stopwords.words('english')\n",
        "text_clean = ' '.join([w for w in text.split(' ') if w not in stop_words])\n",
        "print(\"Removing Stopwords\\n\", text_clean,'\\n')\n",
        "\n",
        "# lowercase\n",
        "text_lower = text.lower()\n",
        "text_clean = ' '.join([w for w in text_lower.split(' ') if w not in stop_words])\n",
        "print(\"Removing Stopwords Again...\\n\", text_clean,'\\n')\n",
        "\n",
        "# punctuation\n",
        "import string\n",
        "print(string.punctuation,'\\n')\n",
        "\n",
        "punctuations = [p for p in string.punctuation]\n",
        "print(punctuations,'\\n')\n",
        "\n",
        "text_clean = ''.join([w for w in text_clean if w not in punctuations])\n",
        "print('Removed punctuations...\\n', text_clean)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removing Stopwords\n",
            " This introduction NLP. It likely useful, people since Machine learning new electricity \n",
            "\n",
            "Removing Stopwords Again...\n",
            " introduction nlp. likely useful, people since machine learning new electricity \n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n",
            "\n",
            "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~'] \n",
            "\n",
            "Removed punctuations...\n",
            " introduction nlp likely useful people since machine learning new electricity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eST67UsulL9",
        "colab_type": "text"
      },
      "source": [
        "Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZnOwGPTrhrA",
        "colab_type": "code",
        "outputId": "4b4b98d6-5af3-48ae-9532-e6551266d986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize  import word_tokenize\n",
        "tokens =   word_tokenize(text_clean)\n",
        "print(tokens)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['introduction', 'nlp', 'likely', 'useful', 'people', 'since', 'machine', 'learning', 'new', 'electricity']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0soExGHk0TiV",
        "colab_type": "text"
      },
      "source": [
        "Stemming and Lemmatizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytW12zJnvBZo",
        "colab_type": "code",
        "outputId": "4d7d0ebe-1082-4047-8e43-97597b964e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print('Porter Stemmer', [ps.stem(w) for w in tokens])\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lema = WordNetLemmatizer()\n",
        "print('Lemmatizer', [lema.lemmatize(w) for w in tokens])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Porter Stemmer ['introduct', 'nlp', 'like', 'use', 'peopl', 'sinc', 'machin', 'learn', 'new', 'electr']\n",
            "Lemmatizer ['introduction', 'nlp', 'likely', 'useful', 'people', 'since', 'machine', 'learning', 'new', 'electricity']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPoJmxSawAXa",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Preprocssing is required to clean our data\n",
        "\n",
        "The techniques we use are:\n",
        "1. lowercase\n",
        "2. stopwords and punctuation\n",
        "3. Stemming or Lemmatizing\n",
        "4. Additional\n",
        "   i.  **Normalizing**\n",
        "   ii. **Correcting spelling**\n",
        "   iii. specific lowercasing, removing stopwords\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2A_lKWX4Gbr",
        "colab_type": "text"
      },
      "source": [
        "## Text to features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRLwfa_fJ9x_",
        "colab_type": "text"
      },
      "source": [
        "Onehot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kkKRbF84I7v",
        "colab_type": "code",
        "outputId": "60a5baef-93d4-4afd-bb29-cde7ce06b4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# one-hot encoding\n",
        "text1 = 'This is introduction to NLP'\n",
        "text2 = 'NLP is Natural Language Processing'\n",
        "# print(text1.split() + text2.split())\n",
        "\n",
        "vocab = set(text1.split() + text2.split())\n",
        "print(vocab)\n",
        "\n",
        "w2idx = dict()\n",
        "for i, w in enumerate(vocab):\n",
        "  w2idx[w] = i \n",
        "print(w2idx)\n",
        "\n",
        "onehot1 = [] \n",
        "onehot2 = []\n",
        "for i in range(len(w2idx)):\n",
        "  onehot1.append(0)\n",
        "  onehot2.append(0)\n",
        "\n",
        "for w in text1.split():\n",
        "  if w in w2idx.keys():\n",
        "    onehot1[w2idx[w]] = 1\n",
        "print(onehot1)\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'This', 'NLP', 'Processing', 'is', 'to', 'introduction', 'Natural', 'Language'}\n",
            "{'This': 0, 'NLP': 1, 'Processing': 2, 'is': 3, 'to': 4, 'introduction': 5, 'Natural': 6, 'Language': 7}\n",
            "[1, 1, 0, 1, 1, 1, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Nrt55cKAIn",
        "colab_type": "text"
      },
      "source": [
        "CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oefvLLX6Esf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = [\"I love NLP and I will learn NLP from today\"]\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(text)\n",
        "vector = vectorizer.transform(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH31tBahFYqu",
        "colab_type": "code",
        "outputId": "9809ed95-8d17-4fb1-9d97-cc998dcea213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print (vectorizer.get_feature_names())\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'from', 'learn', 'love', 'nlp', 'today', 'will']\n",
            "{'love': 3, 'nlp': 4, 'and': 0, 'will': 6, 'learn': 2, 'from': 1, 'today': 5}\n",
            "[[1 1 1 1 2 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKmoLotRLCPK",
        "colab_type": "text"
      },
      "source": [
        "TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I1MC1TVGDa5",
        "colab_type": "code",
        "outputId": "7d5cc90b-cadd-41e3-ffc0-16387fac8ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "\"The dog.\",\n",
        "\"The fox\"]\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(text)\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWCU0wbkMVDj",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJTeoGlUFi80",
        "colab_type": "text"
      },
      "source": [
        "Part of speech tagging\n",
        "\n",
        "Terms\n",
        "\n",
        "- PRP personal pronoun I, he, she\n",
        "- VBP verb, sing. present, non-3d take\n",
        "- CC coordinating conjunction\n",
        "- MD modal could, will\n",
        "- VB verb, base form take\n",
        "- NNP proper noun, singular ‘Harrison’\n",
        "- IN preposition/subordinating conjunction\n",
        "- NN noun, singular ‘desk’\n",
        "\n",
        "More tags: https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUafHvM7LQhi",
        "colab_type": "code",
        "outputId": "d24227d0-19fc-4ff0-dad0-6ff7ebe8d9a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = 'I love NLP and I will learn NLP from today'\n",
        "tokens = sent_tokenize(text)\n",
        "print(tokens)\n",
        "words = nltk.word_tokenize(tokens[0])\n",
        "print(words)\n",
        "print (nltk.pos_tag(words))\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "['I love NLP and I will learn NLP from today']\n",
            "['I', 'love', 'NLP', 'and', 'I', 'will', 'learn', 'NLP', 'from', 'today']\n",
            "[('I', 'PRP'), ('love', 'VBP'), ('NLP', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('will', 'MD'), ('learn', 'VB'), ('NLP', 'NNP'), ('from', 'IN'), ('today', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T61kOqicI8tA",
        "colab_type": "text"
      },
      "source": [
        "Entites Extractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_wHoGknMwzl",
        "colab_type": "code",
        "outputId": "21816a21-32da-43af-d689-44a19d907c2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "from nltk import ne_chunk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "text = \"Sanjay is studying at Sunway College in Kathmandu\"\n",
        "print(ne_chunk(nltk.pos_tag(word_tokenize(text))))\n",
        "\n",
        "text = \"John is studying at Sunway College in Kathmandu\"\n",
        "print(ne_chunk(nltk.pos_tag(word_tokenize(text))))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  (GPE Sanjay/NNP)\n",
            "  is/VBZ\n",
            "  studying/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Sunway/NNP College/NNP)\n",
            "  in/IN\n",
            "  (GPE Kathmandu/NNP))\n",
            "(S\n",
            "  (PERSON John/NNP)\n",
            "  is/VBZ\n",
            "  studying/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Sunway/NNP College/NNP)\n",
            "  in/IN\n",
            "  (GPE Kathmandu/NNP))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4nbv69wKvAq",
        "colab_type": "text"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_bFSXsiK82s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_email = pd.read_csv(\"spam.csv\",encoding ='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpW8JqV2Luit",
        "colab_type": "code",
        "outputId": "78125486-4a75-4ead-e924-a011d7dec6a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_email.columns"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaBhkmuOL1T0",
        "colab_type": "code",
        "outputId": "92907eeb-4522-4a2e-8e23-a05950530867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "df_email.info()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 5 columns):\n",
            "v1            5572 non-null object\n",
            "v2            5572 non-null object\n",
            "Unnamed: 2    50 non-null object\n",
            "Unnamed: 3    12 non-null object\n",
            "Unnamed: 4    6 non-null object\n",
            "dtypes: object(5)\n",
            "memory usage: 217.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5FoV7gPL4gU",
        "colab_type": "code",
        "outputId": "018ceaf8-8a6c-4ac1-96c9-f85591877cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_email.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     v1  ... Unnamed: 4\n",
              "0   ham  ...        NaN\n",
              "1   ham  ...        NaN\n",
              "2  spam  ...        NaN\n",
              "3   ham  ...        NaN\n",
              "4   ham  ...        NaN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg03ij3KL-Dp",
        "colab_type": "code",
        "outputId": "34e24a3d-d44f-4c0a-f374-4d4ec56577a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_email = df_email[['v1','v2']]\n",
        "df_email = df_email.rename(columns={'v1': 'label','v2':'email'})\n",
        "df_email.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>email</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                              email\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU5ARd3jMZRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "\n",
        "df_email['email'] = df_email['email'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "stop = stopwords.words('english')\n",
        "df_email['email'] = df_email['email'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "st = PorterStemmer()\n",
        "df_email['email_stemmed'] = df_email['email'].apply(lambda x:\" \".join ([st.stem(word) for word in x.split()]))\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "df_email['email_lema'] = df_email['email'].apply(lambda x:\" \".join ([lem.lemmatize(word) for word in x.split()]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOGlCc2ojvyX",
        "colab_type": "code",
        "outputId": "37820d01-b912-46a4-f0ac-dd92665ee11b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_email.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>email</th>\n",
              "      <th>email_stemmed</th>\n",
              "      <th>email_lema</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>go jurong point, crazy.. available bugis n gre...</td>\n",
              "      <td>go jurong point, crazy.. avail bugi n great wo...</td>\n",
              "      <td>go jurong point, crazy.. available bugis n gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>ok lar... joking wif u oni...</td>\n",
              "      <td>ok lar... joke wif u oni...</td>\n",
              "      <td>ok lar... joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
              "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
              "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>u dun say early hor... u c already say...</td>\n",
              "      <td>u dun say earli hor... u c alreadi say...</td>\n",
              "      <td>u dun say early hor... u c already say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>nah think goes usf, lives around though</td>\n",
              "      <td>nah think goe usf, live around though</td>\n",
              "      <td>nah think go usf, life around though</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                         email_lema\n",
              "0   ham  ...  go jurong point, crazy.. available bugis n gre...\n",
              "1   ham  ...                      ok lar... joking wif u oni...\n",
              "2  spam  ...  free entry 2 wkly comp win fa cup final tkts 2...\n",
              "3   ham  ...          u dun say early hor... u c already say...\n",
              "4   ham  ...               nah think go usf, life around though\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf_a3Gcjn9Oe",
        "colab_type": "text"
      },
      "source": [
        "Porter and CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YchE1T6maTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using stemmed data\n",
        "column = 'email_stemmed'\n",
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(df_email[column], df_email['label'])\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.fit_transform(test_y)\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "count_vect.fit(df_email[column])\n",
        "xtrain_cv =  count_vect.transform(train_x)\n",
        "xtest_cv =  count_vect.transform(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSrILrogm5AZ",
        "colab_type": "code",
        "outputId": "d5696a2e-f141-44c7-8c9b-34af7b9f9331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "clf = naive_bayes.MultinomialNB(alpha=0.2)\n",
        "clf.fit(xtrain_cv, train_y)\n",
        "pred = clf.predict(xtest_cv)\n",
        "acc = metrics.accuracy_score(pred, test_y)\n",
        "print('navie bayes', acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "navie bayes 0.9777458722182341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DIZ_BBzuJ_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example=df_email.iloc[10]['email']\n",
        "example_cv=count_vect.transform([example])\n",
        "label = df_email.iloc[10]['label']\n",
        "label_e=encoder.transform([label])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBCJU1NjwBU7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cf44172-2f86-458c-f022-2227c6b909f0"
      },
      "source": [
        "print(clf.predict(example_cv))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQhV5vyhwPT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5qPukSyqkgS",
        "colab_type": "code",
        "outputId": "304b6202-d918-415b-ef8e-d1ef0ddf3f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "clf = linear_model.LogisticRegression()\n",
        "clf.fit(xtrain_cv, train_y)\n",
        "pred = clf.predict(xtest_cv)\n",
        "acc = metrics.accuracy_score(pred, test_y)\n",
        "print('logistic regression', acc)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression 0.9834888729361091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIOHxQzSoKnz",
        "colab_type": "text"
      },
      "source": [
        "Lemmatization and CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGicihA2ojzq",
        "colab_type": "code",
        "outputId": "b05a85a0-1e4c-45f3-a84a-df9cc4524498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# using stemmed data\n",
        "column = 'email_lema'\n",
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(df_email[column], df_email['label'])\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.fit_transform(test_y)\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "count_vect.fit(df_email[column])\n",
        "xtrain_cv =  count_vect.transform(train_x)\n",
        "xtest_cv =  count_vect.transform(test_x)\n",
        "\n",
        "clf = naive_bayes.MultinomialNB(alpha=0.2)\n",
        "clf.fit(xtrain_cv, train_y)\n",
        "pred = clf.predict(xtest_cv)\n",
        "acc = metrics.accuracy_score(pred, test_y)\n",
        "print(acc)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9806173725771715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR-zGKwGrFRO",
        "colab_type": "code",
        "outputId": "65d377dd-b88a-42ac-cfe8-6a65983efc92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "clf = linear_model.LogisticRegression()\n",
        "clf.fit(xtrain_cv, train_y)\n",
        "pred = clf.predict(xtest_cv)\n",
        "acc = metrics.accuracy_score(pred, test_y)\n",
        "print('logistic regression', acc)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression 0.9827709978463748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsPkNm0Wq5W3",
        "colab_type": "code",
        "outputId": "d1a10cf1-4042-4c80-d988-2870589540a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "clf = linear_model.LogisticRegression()\n",
        "clf.fit(xtrain_cv, train_y)\n",
        "pred = clf.predict(xtest_cv)\n",
        "acc = metrics.accuracy_score(pred, test_y)\n",
        "print('logistic regression', acc)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression 0.9827709978463748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA0vVTPBmXDf",
        "colab_type": "text"
      },
      "source": [
        "Porter and TfIdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdSKTNt_kVky",
        "colab_type": "code",
        "outputId": "6336d8d4-ad1c-43fa-9b67-17bec59a93a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# using stemmed data\n",
        "column = 'email_stemmed'\n",
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(df_email[column], df_email['label'])\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.fit_transform(test_y)\n",
        "# tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect.fit(df_email[column])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xtest_tfidf =  tfidf_vect.transform(test_x)\n",
        "\n",
        "clf = naive_bayes.MultinomialNB(alpha=0.2)\n",
        "clf.fit(xtrain_tfidf, train_y)\n",
        "pred = clf.predict(xtest_tfidf)\n",
        "acc = metrics.accuracy_score(pred, test_y)\n",
        "print(acc)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9863603732950467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7iAtVaJrOdP",
        "colab_type": "code",
        "outputId": "d585ab53-524e-46b2-bf4a-d46004345e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "clf = linear_model.LogisticRegression()\n",
        "clf.fit(xtrain_tfidf, train_y)\n",
        "pred = clf.predict(xtest_tfidf)\n",
        "acc = metrics.accuracy_score(pred, test_y)\n",
        "print('logistic regression', acc)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression 0.9547738693467337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_oxA2suo5u9",
        "colab_type": "text"
      },
      "source": [
        "Lema and TfIdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYrJ5Ut0lbGn",
        "colab_type": "code",
        "outputId": "42d84f13-ffb3-46f1-d94e-509de4ad7068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# using stemmed data\n",
        "column = 'email_lema'\n",
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(df_email[column], df_email['label'])\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.fit_transform(test_y)\n",
        "\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect.fit(df_email[column])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xtest_tfidf =  tfidf_vect.transform(test_x)\n",
        "\n",
        "clf = naive_bayes.MultinomialNB(alpha=0.2)\n",
        "clf.fit(xtrain_tfidf, train_y)\n",
        "pred = clf.predict(xtest_tfidf)\n",
        "acc = metrics.accuracy_score(pred, test_y)\n",
        "print(acc)\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9784637473079684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWMwJrPdrtO0",
        "colab_type": "code",
        "outputId": "217cb8d6-ba8c-498a-f894-a72963de211d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "clf = linear_model.LogisticRegression()\n",
        "clf.fit(xtrain_tfidf, train_y)\n",
        "pred = clf.predict(xtest_tfidf)\n",
        "acc = metrics.accuracy_score(pred, test_y)\n",
        "print('logistic regression', acc)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression 0.9504666188083274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrxGxhT0mNTW",
        "colab_type": "code",
        "outputId": "1becae32-7865-40da-db10-8ed233fe032d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "clf.predict(tfidf_vect.transform([\"win lottery. be rich hurry up discount offer\"]))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdResFoHr9s0",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "As we can see that the result varries they way we choose to represent our text or the model we choose.\n",
        "\n",
        "There isn't one solution that applies for all. You have to test out different approach and use that works best for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y9sAyDosxTK",
        "colab_type": "text"
      },
      "source": [
        "## Assignment\n",
        "\n",
        "Carry out sentiment analysis on ISEAR data. \n",
        "\n",
        "- Try different preprocessing techniques\n",
        "- Use different machine learning models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG5PbEs8weJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}