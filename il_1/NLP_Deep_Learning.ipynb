{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Deep Learning",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagunkayastha/ILab_Tutorials/blob/master/NLP_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ok_ss41G7-N",
        "colab_type": "text"
      },
      "source": [
        "# Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhTdvwLM41LZ",
        "colab_type": "text"
      },
      "source": [
        "**Downloading and extracting word embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8nCcmNSQlSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kydniaTG4Zb",
        "colab_type": "code",
        "outputId": "d625e4fe-b516-49f4-ed64-69a4afe4fa3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
        "!unzip glove.42B.300d.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-27 04:16:45--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2019-09-27 04:16:45--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2019-09-27 04:16:45--  http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  83.2MB/s    in 29s     \n",
            "\n",
            "2019-09-27 04:17:14 (61.9 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n",
            "Archive:  glove.42B.300d.zip\n",
            "  inflating: glove.42B.300d.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WxqqScfHA8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        \n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "            \n",
        "    return words, word_to_vec_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOFvTcx1kStp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words, word_to_vec_map = read_glove_vecs('glove.42B.300d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMSD6KGt2BQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "father = word_to_vec_map[\"father\"]\n",
        "mother = word_to_vec_map[\"mother\"]\n",
        "king = word_to_vec_map[\"king\"]\n",
        "queen = word_to_vec_map[\"queen\"]\n",
        "france = word_to_vec_map[\"france\"]\n",
        "italy = word_to_vec_map[\"italy\"]\n",
        "paris = word_to_vec_map[\"paris\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DphLALHm5UxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the embedding\n",
        "print(father)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1ewSjIe5goq",
        "colab_type": "text"
      },
      "source": [
        "**Cosine Similarity**\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2Y5GoOI2WX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \n",
        "    distance = 0.0\n",
        "    \n",
        "    dot = np.dot(u, v)\n",
        "    \n",
        "    norm_u = np.sqrt(np.sum(u**2))\n",
        "    norm_v = np.sqrt(np.sum(v**2))\n",
        "    \n",
        "    cosine_similarity = dot / np.dot(norm_u, norm_v)\n",
        "\n",
        "    \n",
        "    return cosine_similarity\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyVrcHmH4wwQ",
        "colab_type": "code",
        "outputId": "0a4e9e00-3e15-4258-c95a-9ae48d564288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(cosine_similarity(father,mother))\n",
        "print(cosine_similarity(king,queen))\n",
        "print(cosine_similarity(king,rome))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8171303880464806\n",
            "0.7596175811305904\n",
            "0.39602592048853846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec7Ytrfm56Hc",
        "colab_type": "text"
      },
      "source": [
        "**A=B then C=?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVnATueR3zd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
        "    \n",
        "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
        "    \n",
        "    \n",
        "    # word embeddings v_a, v_b and v_c \n",
        "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
        "    \n",
        "    \n",
        "    words = word_to_vec_map.keys()\n",
        "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
        "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
        "\n",
        "    # loop over the whole word vector set\n",
        "    for w in words:        \n",
        "        \n",
        "        if w in [word_a, word_b, word_c] :\n",
        "            continue\n",
        "        \n",
        "        \n",
        "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
        "        cosine_sim = cosine_similarity((e_b - e_a), (word_to_vec_map[w] - e_c))\n",
        "        \n",
        "       \n",
        "        if cosine_sim > max_cosine_sim:\n",
        "            max_cosine_sim = cosine_sim\n",
        "            best_word = w\n",
        "        \n",
        "        \n",
        "    return best_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Au1uXIM4BbM",
        "colab_type": "code",
        "outputId": "e4339f4a-4b9b-4502-e74a-fbaf7d91d311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "complete_analogy('France','Paris','Germany',word_to_vec_map)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'burger'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3XiCYnLfXbQ",
        "colab_type": "text"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP-RUNCHV-wN",
        "colab_type": "code",
        "outputId": "62bc904c-4b46-43c1-981d-65ae159718e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sagunkayastha/ILab_Tutorials/master/ISEAR.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-26 16:13:48--  https://raw.githubusercontent.com/sagunkayastha/ILab_Tutorials/master/ISEAR.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 957909 (935K) [text/plain]\n",
            "Saving to: ‘ISEAR.csv.2’\n",
            "\n",
            "\rISEAR.csv.2           0%[                    ]       0  --.-KB/s               \rISEAR.csv.2         100%[===================>] 935.46K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2019-09-26 16:13:48 (106 MB/s) - ‘ISEAR.csv.2’ saved [957909/957909]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br62dSUYSiyd",
        "colab_type": "code",
        "outputId": "87d3e630-22d0-43b9-f709-b074d4f75155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# load and preprocess the dataset\n",
        "df = pd.read_csv('./ISEAR.csv',names = ['Emotions','Sentence'], index_col=False)\n",
        "df.describe()\n",
        "\n",
        "# preprocessing the dataset\n",
        "\n",
        "# clean the dataset\n",
        "df.dropna(axis=0, inplace = True)\n",
        "df.describe()\n",
        "# replacing misspelled data\n",
        "df['Emotions'].replace(to_replace = 'guit', value='guilt', inplace = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiVayqbYS0pT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    sw = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from','in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y']\n",
        "    text = text.lower()\n",
        "    text = nltk.word_tokenize(text)\n",
        "    stem = WordNetLemmatizer()\n",
        "    text = [stem.lemmatize(w) for w in text if w not in sw and w.isalnum()]\n",
        "    text = ' '.join(text)\n",
        "    return text\n",
        "df['Sentence'] = df['Sentence'].map(lambda x: clean_text(x))\n",
        "df.drop(df[df['Sentence'].apply(len) == 0].index, inplace=True)\n",
        "\n",
        "# check the dataset after cleaning\n",
        "# print(df['Emotions'].value_counts())\n",
        "\n",
        "# split the dataset\n",
        "X, y = df['Sentence'], df['Emotions']\n",
        "\n",
        "# transform y to one-hot-encoding\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "encoded_y = encoder.transform(y)\n",
        "onehot_y = np_utils.to_categorical(encoded_y)\n",
        "\n",
        "# total number of unique words\n",
        "vocab = set()\n",
        "total_vocab = [vocab.add(el) for s in X.values for el in s.split(' ')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy-GAzMSHaKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embeddings_index = {}\n",
        "# f = open('glove.42B.300d.txt', encoding='utf8')\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     word = ''.join(values[:-300])\n",
        "#     coefs = np.asarray(values[-300:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qFimP1qTjT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize the sentences\n",
        "VOCABULARY_SIZE = len(total_vocab)  # Select an Appropriate Vobabulary Size\n",
        "PADDED_LENGTH = 90  # Select an Appropriate padded Length for text\n",
        "tokenizer = Tokenizer(VOCABULARY_SIZE) \n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# text to number\n",
        "def preprocessing(X):\n",
        "    seq = tokenizer.texts_to_sequences(X)\n",
        "    data = sequence.pad_sequences(seq, maxlen = PADDED_LENGTH)\n",
        "    return data\n",
        "data = preprocessing(X)\n",
        "\n",
        "\n",
        "# split test set and train set\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, onehot_y, test_size = 0.1, random_state = 101, stratify = onehot_y)\n",
        "\n",
        "# model\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCH = 80\n",
        "EMBEDDING_SIZE = 300\n",
        "SEED = 2019\n",
        "DROPOUTRATE = 0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feT64JX4QcU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = word_to_vec_map\n",
        "embedding_matrix = np.zeros((VOCABULARY_SIZE, 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQkZH-X2Rhnj",
        "colab_type": "code",
        "outputId": "db573c4b-7906-421b-9c6a-00b977d61d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "\n",
        "modelrnn = Sequential()\n",
        "modelrnn.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length = PADDED_LENGTH,weights=[embedding_matrix], trainable= 'false',\n",
        "                   embeddings_regularizer=regularizers.l1(0.001)))\n",
        "modelrnn.add(SpatialDropout1D(0.2))\n",
        "\n",
        "\n",
        "modelrnn.add(LSTM(128, dropout=DROPOUTRATE, recurrent_dropout=DROPOUTRATE, return_sequences = True))\n",
        "modelrnn.add(LSTM(64, dropout=DROPOUTRATE, recurrent_dropout=DROPOUTRATE))\n",
        "modelrnn.add(Dense(64, activation='relu'))\n",
        "modelrnn.add(Dropout(DROPOUTRATE, seed = SEED))\n",
        "modelrnn.add(Dense(128, activation='relu'))\n",
        "modelrnn.add(Dropout(DROPOUTRATE, seed = SEED))\n",
        "modelrnn.add(Dense(7, activation='softmax'))\n",
        "\n",
        "optim = optimizers.Adam(lr = LEARNING_RATE,\n",
        "                       beta_1=0.9,\n",
        "                       beta_2=0.999,\n",
        "                       epsilon=1e-08,\n",
        "                       decay=LEARNING_RATE/EPOCH)\n",
        "\n",
        "modelrnn.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "modelrnn.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 90, 300)           22623600  \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 90, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 90, 128)           219648    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 22,906,039\n",
            "Trainable params: 22,906,039\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu3hdE0JUiN7",
        "colab_type": "code",
        "outputId": "1f7643c8-14fd-4e5a-8206-dc29707c891f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "history = modelrnn.fit(X_train,y_train, batch_size=16, validation_split=0.2, epochs = EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 5410 samples, validate on 1353 samples\n",
            "Epoch 1/80\n",
            "5410/5410 [==============================] - 132s 24ms/step - loss: 356.2661 - acc: 0.2362 - val_loss: 157.8668 - val_acc: 0.3341\n",
            "Epoch 2/80\n",
            "5410/5410 [==============================] - 129s 24ms/step - loss: 83.3340 - acc: 0.3299 - val_loss: 40.1806 - val_acc: 0.3865\n",
            "Epoch 3/80\n",
            "5410/5410 [==============================] - 129s 24ms/step - loss: 27.3153 - acc: 0.3743 - val_loss: 19.5375 - val_acc: 0.3851\n",
            "Epoch 4/80\n",
            "5410/5410 [==============================] - 130s 24ms/step - loss: 16.2753 - acc: 0.4083 - val_loss: 13.5264 - val_acc: 0.4671\n",
            "Epoch 5/80\n",
            "5410/5410 [==============================] - 129s 24ms/step - loss: 11.6380 - acc: 0.4335 - val_loss: 9.8552 - val_acc: 0.4568\n",
            "Epoch 6/80\n",
            "5410/5410 [==============================] - 129s 24ms/step - loss: 8.5374 - acc: 0.4671 - val_loss: 7.1794 - val_acc: 0.4982\n",
            "Epoch 7/80\n",
            "5410/5410 [==============================] - 129s 24ms/step - loss: 6.2841 - acc: 0.4858 - val_loss: 5.3736 - val_acc: 0.5144\n",
            "Epoch 8/80\n",
            "5410/5410 [==============================] - 129s 24ms/step - loss: 4.7653 - acc: 0.4952 - val_loss: 4.1743 - val_acc: 0.4982\n",
            "Epoch 9/80\n",
            "5410/5410 [==============================] - 129s 24ms/step - loss: 3.7603 - acc: 0.5078 - val_loss: 3.3481 - val_acc: 0.5366\n",
            "Epoch 10/80\n",
            "5410/5410 [==============================] - 129s 24ms/step - loss: 3.1884 - acc: 0.5248 - val_loss: 3.0302 - val_acc: 0.5211\n",
            "Epoch 11/80\n",
            "5410/5410 [==============================] - 129s 24ms/step - loss: 2.9214 - acc: 0.5292 - val_loss: 2.6988 - val_acc: 0.5285\n",
            "Epoch 12/80\n",
            "5410/5410 [==============================] - 128s 24ms/step - loss: 2.7499 - acc: 0.5351 - val_loss: 2.6994 - val_acc: 0.5373\n",
            "Epoch 13/80\n",
            "5410/5410 [==============================] - 127s 23ms/step - loss: 2.6508 - acc: 0.5512 - val_loss: 2.5721 - val_acc: 0.5455\n",
            "Epoch 14/80\n",
            "5410/5410 [==============================] - 127s 23ms/step - loss: 2.5606 - acc: 0.5492 - val_loss: 2.6155 - val_acc: 0.5410\n",
            "Epoch 15/80\n",
            "5410/5410 [==============================] - 127s 23ms/step - loss: 2.5543 - acc: 0.5556 - val_loss: 2.5415 - val_acc: 0.5388\n",
            "Epoch 16/80\n",
            "5410/5410 [==============================] - 127s 23ms/step - loss: 2.4750 - acc: 0.5704 - val_loss: 2.5260 - val_acc: 0.5381\n",
            "Epoch 17/80\n",
            "5410/5410 [==============================] - 127s 24ms/step - loss: 2.5152 - acc: 0.5673 - val_loss: 2.5697 - val_acc: 0.5307\n",
            "Epoch 18/80\n",
            "5410/5410 [==============================] - 124s 23ms/step - loss: 2.4862 - acc: 0.5786 - val_loss: 2.5269 - val_acc: 0.5528\n",
            "Epoch 19/80\n",
            "5410/5410 [==============================] - 121s 22ms/step - loss: 2.4949 - acc: 0.5872 - val_loss: 2.5457 - val_acc: 0.5558\n",
            "Epoch 20/80\n",
            "5410/5410 [==============================] - 118s 22ms/step - loss: 2.4424 - acc: 0.5913 - val_loss: 2.5163 - val_acc: 0.5551\n",
            "Epoch 21/80\n",
            "5410/5410 [==============================] - 117s 22ms/step - loss: 2.4233 - acc: 0.5978 - val_loss: 2.5729 - val_acc: 0.5521\n",
            "Epoch 22/80\n",
            "5410/5410 [==============================] - 116s 21ms/step - loss: 2.4477 - acc: 0.5959 - val_loss: 2.5132 - val_acc: 0.5477\n",
            "Epoch 23/80\n",
            "5410/5410 [==============================] - 115s 21ms/step - loss: 2.4160 - acc: 0.6002 - val_loss: 2.4956 - val_acc: 0.5528\n",
            "Epoch 24/80\n",
            "5410/5410 [==============================] - 114s 21ms/step - loss: 2.4135 - acc: 0.6122 - val_loss: 2.5931 - val_acc: 0.5506\n",
            "Epoch 25/80\n",
            "5410/5410 [==============================] - 114s 21ms/step - loss: 2.3911 - acc: 0.6150 - val_loss: 2.5619 - val_acc: 0.5506\n",
            "Epoch 26/80\n",
            "5410/5410 [==============================] - 114s 21ms/step - loss: 2.4149 - acc: 0.6181 - val_loss: 2.6725 - val_acc: 0.5314\n",
            "Epoch 27/80\n",
            "5410/5410 [==============================] - 114s 21ms/step - loss: 2.4366 - acc: 0.6250 - val_loss: 2.5549 - val_acc: 0.5410\n",
            "Epoch 28/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.4351 - acc: 0.6227 - val_loss: 2.5880 - val_acc: 0.5492\n",
            "Epoch 29/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.4036 - acc: 0.6292 - val_loss: 2.5960 - val_acc: 0.5477\n",
            "Epoch 30/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3759 - acc: 0.6412 - val_loss: 2.6371 - val_acc: 0.5492\n",
            "Epoch 31/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.4160 - acc: 0.6322 - val_loss: 2.6675 - val_acc: 0.5551\n",
            "Epoch 32/80\n",
            "5410/5410 [==============================] - 120s 22ms/step - loss: 2.4218 - acc: 0.6381 - val_loss: 2.5978 - val_acc: 0.5602\n",
            "Epoch 33/80\n",
            "5410/5410 [==============================] - 120s 22ms/step - loss: 2.3999 - acc: 0.6409 - val_loss: 2.6246 - val_acc: 0.5654\n",
            "Epoch 34/80\n",
            "5410/5410 [==============================] - 118s 22ms/step - loss: 2.3975 - acc: 0.6468 - val_loss: 2.6960 - val_acc: 0.5514\n",
            "Epoch 35/80\n",
            "5410/5410 [==============================] - 117s 22ms/step - loss: 2.3805 - acc: 0.6495 - val_loss: 2.7244 - val_acc: 0.5521\n",
            "Epoch 36/80\n",
            "5410/5410 [==============================] - 116s 21ms/step - loss: 2.3795 - acc: 0.6627 - val_loss: 2.6425 - val_acc: 0.5558\n",
            "Epoch 37/80\n",
            "5410/5410 [==============================] - 115s 21ms/step - loss: 2.3595 - acc: 0.6529 - val_loss: 2.6522 - val_acc: 0.5706\n",
            "Epoch 38/80\n",
            "5410/5410 [==============================] - 115s 21ms/step - loss: 2.3663 - acc: 0.6588 - val_loss: 2.6584 - val_acc: 0.5728\n",
            "Epoch 39/80\n",
            "5410/5410 [==============================] - 114s 21ms/step - loss: 2.3922 - acc: 0.6571 - val_loss: 2.6653 - val_acc: 0.5573\n",
            "Epoch 40/80\n",
            "5410/5410 [==============================] - 115s 21ms/step - loss: 2.3359 - acc: 0.6678 - val_loss: 2.6495 - val_acc: 0.5588\n",
            "Epoch 41/80\n",
            "5410/5410 [==============================] - 114s 21ms/step - loss: 2.3417 - acc: 0.6643 - val_loss: 2.7365 - val_acc: 0.5639\n",
            "Epoch 42/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3783 - acc: 0.6660 - val_loss: 2.7720 - val_acc: 0.5580\n",
            "Epoch 43/80\n",
            "5410/5410 [==============================] - 114s 21ms/step - loss: 2.3722 - acc: 0.6767 - val_loss: 2.8038 - val_acc: 0.5743\n",
            "Epoch 44/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3580 - acc: 0.6769 - val_loss: 2.7830 - val_acc: 0.5661\n",
            "Epoch 45/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3183 - acc: 0.6839 - val_loss: 2.6842 - val_acc: 0.5647\n",
            "Epoch 46/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3094 - acc: 0.6750 - val_loss: 2.7087 - val_acc: 0.5654\n",
            "Epoch 47/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3223 - acc: 0.6872 - val_loss: 2.6698 - val_acc: 0.5669\n",
            "Epoch 48/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3246 - acc: 0.6800 - val_loss: 2.6998 - val_acc: 0.5698\n",
            "Epoch 49/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3314 - acc: 0.6891 - val_loss: 2.7605 - val_acc: 0.5765\n",
            "Epoch 50/80\n",
            "5410/5410 [==============================] - 114s 21ms/step - loss: 2.3320 - acc: 0.6880 - val_loss: 2.7870 - val_acc: 0.5802\n",
            "Epoch 51/80\n",
            "5410/5410 [==============================] - 115s 21ms/step - loss: 2.3602 - acc: 0.6911 - val_loss: 2.8024 - val_acc: 0.5706\n",
            "Epoch 52/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2804 - acc: 0.6989 - val_loss: 2.7200 - val_acc: 0.5780\n",
            "Epoch 53/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3152 - acc: 0.7006 - val_loss: 2.7919 - val_acc: 0.5824\n",
            "Epoch 54/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3390 - acc: 0.6989 - val_loss: 2.8116 - val_acc: 0.5780\n",
            "Epoch 55/80\n",
            "5410/5410 [==============================] - 112s 21ms/step - loss: 2.2904 - acc: 0.7068 - val_loss: 2.8287 - val_acc: 0.5617\n",
            "Epoch 56/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.3028 - acc: 0.7044 - val_loss: 2.7361 - val_acc: 0.5809\n",
            "Epoch 57/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2965 - acc: 0.6974 - val_loss: 2.7594 - val_acc: 0.5721\n",
            "Epoch 58/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2840 - acc: 0.7122 - val_loss: 2.7720 - val_acc: 0.5676\n",
            "Epoch 59/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2639 - acc: 0.7087 - val_loss: 2.8211 - val_acc: 0.5735\n",
            "Epoch 60/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2682 - acc: 0.7100 - val_loss: 2.8212 - val_acc: 0.5654\n",
            "Epoch 61/80\n",
            "5410/5410 [==============================] - 112s 21ms/step - loss: 2.2646 - acc: 0.7198 - val_loss: 2.7718 - val_acc: 0.5661\n",
            "Epoch 62/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2782 - acc: 0.7129 - val_loss: 2.8477 - val_acc: 0.5780\n",
            "Epoch 63/80\n",
            "5410/5410 [==============================] - 112s 21ms/step - loss: 2.2889 - acc: 0.7185 - val_loss: 2.8054 - val_acc: 0.5787\n",
            "Epoch 64/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2690 - acc: 0.7155 - val_loss: 2.7681 - val_acc: 0.5854\n",
            "Epoch 65/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2202 - acc: 0.7331 - val_loss: 2.8339 - val_acc: 0.5706\n",
            "Epoch 66/80\n",
            "5410/5410 [==============================] - 112s 21ms/step - loss: 2.2299 - acc: 0.7229 - val_loss: 2.7874 - val_acc: 0.5787\n",
            "Epoch 67/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2784 - acc: 0.7261 - val_loss: 2.9511 - val_acc: 0.5721\n",
            "Epoch 68/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2794 - acc: 0.7279 - val_loss: 2.9514 - val_acc: 0.5706\n",
            "Epoch 69/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2864 - acc: 0.7316 - val_loss: 2.9598 - val_acc: 0.5661\n",
            "Epoch 70/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2937 - acc: 0.7320 - val_loss: 2.8473 - val_acc: 0.5698\n",
            "Epoch 71/80\n",
            "5410/5410 [==============================] - 112s 21ms/step - loss: 2.2377 - acc: 0.7338 - val_loss: 2.8622 - val_acc: 0.5691\n",
            "Epoch 72/80\n",
            "5410/5410 [==============================] - 112s 21ms/step - loss: 2.2213 - acc: 0.7394 - val_loss: 2.9070 - val_acc: 0.5780\n",
            "Epoch 73/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2041 - acc: 0.7449 - val_loss: 2.9390 - val_acc: 0.5684\n",
            "Epoch 74/80\n",
            "5410/5410 [==============================] - 112s 21ms/step - loss: 2.2413 - acc: 0.7442 - val_loss: 2.9796 - val_acc: 0.5721\n",
            "Epoch 75/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2260 - acc: 0.7383 - val_loss: 2.9363 - val_acc: 0.5654\n",
            "Epoch 76/80\n",
            "5410/5410 [==============================] - 113s 21ms/step - loss: 2.2363 - acc: 0.7436 - val_loss: 2.9121 - val_acc: 0.5684\n",
            "Epoch 77/80\n",
            "3296/5410 [=================>............] - ETA: 42s - loss: 2.1747 - acc: 0.7691"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctINTiUAf-D3",
        "colab_type": "text"
      },
      "source": [
        "# OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22_YLXvDgC8J",
        "colab_type": "code",
        "outputId": "418b5811-0468-4ed5-ca7b-b24d91c37a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2.git\n",
        "%cd gpt-2/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 209, done.\u001b[K\n",
            "Receiving objects:   0% (1/209)   \rReceiving objects:   1% (3/209)   \rReceiving objects:   2% (5/209)   \rReceiving objects:   3% (7/209)   \rReceiving objects:   4% (9/209)   \rReceiving objects:   5% (11/209)   \rReceiving objects:   6% (13/209)   \rReceiving objects:   7% (15/209)   \rReceiving objects:   8% (17/209)   \rReceiving objects:   9% (19/209)   \rReceiving objects:  10% (21/209)   \rReceiving objects:  11% (23/209)   \rReceiving objects:  12% (26/209)   \rReceiving objects:  13% (28/209)   \rReceiving objects:  14% (30/209)   \rReceiving objects:  15% (32/209)   \rReceiving objects:  16% (34/209)   \rReceiving objects:  17% (36/209)   \rReceiving objects:  18% (38/209)   \rReceiving objects:  19% (40/209)   \rReceiving objects:  20% (42/209)   \rReceiving objects:  21% (44/209)   \rReceiving objects:  22% (46/209)   \rReceiving objects:  23% (49/209)   \rReceiving objects:  24% (51/209)   \rReceiving objects:  25% (53/209)   \rReceiving objects:  26% (55/209)   \rReceiving objects:  27% (57/209)   \rReceiving objects:  28% (59/209)   \rReceiving objects:  29% (61/209)   \rremote: Total 209 (delta 0), reused 0 (delta 0), pack-reused 209\u001b[K\n",
            "Receiving objects:  30% (63/209)   \rReceiving objects:  31% (65/209)   \rReceiving objects:  32% (67/209)   \rReceiving objects:  33% (69/209)   \rReceiving objects:  34% (72/209)   \rReceiving objects:  35% (74/209)   \rReceiving objects:  36% (76/209)   \rReceiving objects:  37% (78/209)   \rReceiving objects:  38% (80/209)   \rReceiving objects:  39% (82/209)   \rReceiving objects:  40% (84/209)   \rReceiving objects:  41% (86/209)   \rReceiving objects:  42% (88/209)   \rReceiving objects:  43% (90/209)   \rReceiving objects:  44% (92/209)   \rReceiving objects:  45% (95/209)   \rReceiving objects:  46% (97/209)   \rReceiving objects:  47% (99/209)   \rReceiving objects:  48% (101/209)   \rReceiving objects:  49% (103/209)   \rReceiving objects:  50% (105/209)   \rReceiving objects:  51% (107/209)   \rReceiving objects:  52% (109/209)   \rReceiving objects:  53% (111/209)   \rReceiving objects:  54% (113/209)   \rReceiving objects:  55% (115/209)   \rReceiving objects:  56% (118/209)   \rReceiving objects:  57% (120/209)   \rReceiving objects:  58% (122/209)   \rReceiving objects:  59% (124/209)   \rReceiving objects:  60% (126/209)   \rReceiving objects:  61% (128/209)   \rReceiving objects:  62% (130/209)   \rReceiving objects:  63% (132/209)   \rReceiving objects:  64% (134/209)   \rReceiving objects:  65% (136/209)   \rReceiving objects:  66% (138/209)   \rReceiving objects:  67% (141/209)   \rReceiving objects:  68% (143/209)   \rReceiving objects:  69% (145/209)   \rReceiving objects:  70% (147/209)   \rReceiving objects:  71% (149/209)   \rReceiving objects:  72% (151/209)   \rReceiving objects:  73% (153/209)   \rReceiving objects:  74% (155/209)   \rReceiving objects:  75% (157/209)   \rReceiving objects:  76% (159/209)   \rReceiving objects:  77% (161/209)   \rReceiving objects:  78% (164/209)   \rReceiving objects:  79% (166/209)   \rReceiving objects:  80% (168/209)   \rReceiving objects:  81% (170/209)   \rReceiving objects:  82% (172/209)   \rReceiving objects:  83% (174/209)   \rReceiving objects:  84% (176/209)   \rReceiving objects:  85% (178/209)   \rReceiving objects:  86% (180/209)   \rReceiving objects:  87% (182/209)   \rReceiving objects:  88% (184/209)   \rReceiving objects:  89% (187/209)   \rReceiving objects:  90% (189/209)   \rReceiving objects:  91% (191/209)   \rReceiving objects:  92% (193/209)   \rReceiving objects:  93% (195/209)   \rReceiving objects:  94% (197/209)   \rReceiving objects:  95% (199/209)   \rReceiving objects:  96% (201/209)   \rReceiving objects:  97% (203/209)   \rReceiving objects:  98% (205/209)   \rReceiving objects:  99% (207/209)   \rReceiving objects: 100% (209/209)   \rReceiving objects: 100% (209/209), 4.37 MiB | 15.49 MiB/s, done.\n",
            "Resolving deltas:   0% (0/109)   \rResolving deltas:   6% (7/109)   \rResolving deltas:   7% (8/109)   \rResolving deltas:   8% (9/109)   \rResolving deltas:  11% (12/109)   \rResolving deltas:  12% (14/109)   \rResolving deltas:  16% (18/109)   \rResolving deltas:  17% (19/109)   \rResolving deltas:  19% (21/109)   \rResolving deltas:  20% (22/109)   \rResolving deltas:  25% (28/109)   \rResolving deltas:  27% (30/109)   \rResolving deltas:  32% (35/109)   \rResolving deltas:  36% (40/109)   \rResolving deltas:  41% (45/109)   \rResolving deltas:  46% (51/109)   \rResolving deltas:  48% (53/109)   \rResolving deltas:  49% (54/109)   \rResolving deltas:  54% (59/109)   \rResolving deltas:  66% (72/109)   \rResolving deltas:  67% (74/109)   \rResolving deltas:  69% (76/109)   \rResolving deltas:  71% (78/109)   \rResolving deltas:  74% (81/109)   \rResolving deltas:  80% (88/109)   \rResolving deltas:  82% (90/109)   \rResolving deltas:  88% (96/109)   \rResolving deltas:  90% (99/109)   \rResolving deltas: 100% (109/109)   \rResolving deltas: 100% (109/109), done.\n",
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96tB-eMSi4T3",
        "colab_type": "code",
        "outputId": "dae710d4-9aa3-41d4-a24e-5edf5b382a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.9MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=e888a5bce565753582a2e3b8c77c18a77067f5a30317eb1706ac841e10c28b1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533196 sha256=54ca929d667b58cf6ace272057218c6f215021eea22e584969ce40bd081ed37d\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.2.1 regex-2017.4.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRfBKnIJid4U",
        "colab_type": "code",
        "outputId": "2d054d53-de5f-4c84-9211-39a7b77ab0d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!python download_model.py 124M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 589kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 27.5Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 609kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:09, 50.7Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 3.34Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 35.8Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 26.1Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToVWCWOKinrI",
        "colab_type": "code",
        "outputId": "c35b8ef9-8697-4b58-d470-8ff587410e04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python src/interactive_conditional_samples.py 124M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:57: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-09-26 07:51:24.735194: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-26 07:51:24.787428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-26 07:51:24.788252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-26 07:51:24.794955: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-26 07:51:25.032229: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-26 07:51:25.137095: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-26 07:51:25.168985: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-26 07:51:25.421818: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-26 07:51:25.554309: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-26 07:51:26.027616: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-26 07:51:26.027992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-26 07:51:26.028847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-26 07:51:26.029518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-26 07:51:26.044569: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-09-26 07:51:26.049353: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14572c0 executing computations on platform Host. Devices:\n",
            "2019-09-26 07:51:26.049389: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-26 07:51:26.159070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-26 07:51:26.160035: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1457480 executing computations on platform CUDA. Devices:\n",
            "2019-09-26 07:51:26.160069: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-26 07:51:26.161479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-26 07:51:26.162202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-26 07:51:26.162291: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-26 07:51:26.162335: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-26 07:51:26.162374: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-26 07:51:26.162412: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-26 07:51:26.162449: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-26 07:51:26.162491: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-26 07:51:26.162529: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-26 07:51:26.162662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-26 07:51:26.163431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-26 07:51:26.164126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-26 07:51:26.168973: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-26 07:51:26.170714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-26 07:51:26.170750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-26 07:51:26.170792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-26 07:51:26.179279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-26 07:51:26.180187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-26 07:51:26.180895: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-09-26 07:51:26.180994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:58: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:60: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:39: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:68: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> I was walking and then\n",
            "2019-09-26 07:51:49.822682: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            " we realized perfectly well that every time they got into front of me, it was no place for sex,\" Hwang said. \"All the women in the bar room walked away feeling mischievous but a bit sad because they didn't know how to actively stop them.\"\n",
            "\n",
            "Hwang added, \"I saw a staff member walking back toward us and she said maybe we should be joking with the men watching the guys are going down and come back up to stop us.\"\n",
            "\n",
            "Interestingly, trends associated with lawmakers seeking ratings on problem bills to validate rules don't always show up within days. According to Thomas Yeardley, Republican Senate candidate for Indiana's 10th congressional district, the 11th grade sexual misconduct complaint that began this year still had to be sent in by October of this year since it hadn't gone off line even before Hwang initiated an official push.\n",
            "\n",
            "\"In the course of that process we flagged some employers and other employers got calls from District 12 citing bigotry and harassing behavior of colleagues,\" Yeardley said in an email. \"They told us they acted civilly but had they known what was going on we would have filed a complaint. We also note that Republicans have often touted our 'pride and duty to our constituents.' The number of response organizations varies under district boundaries. However one of whom contacted the local district attorney's office, and they reported the basis of a tent city infraction thus far. At the end of October, IU's legislators were allowed to seriously analyze 110 incidents, but they did not discuss further specific cases.\"\n",
            "\n",
            "EastCD sends out alerts to local public policy officials if an answer — non-response or formal warnings of threats — is listed on MPOs. This includes potential offenders setting up meetings, grand juries or other informational sessions. It also sends out online updates where province Dps notify legislators of ongoing investigations.\n",
            "\n",
            "EastCD also receives employment and training threat alerts. Very rarely do government officials—even their critics—what they're supposedly doing.\n",
            "\n",
            "In this instance, Andrew Dean, a Republican organization seeking public education campaigns from non-profit The Truth About Classroom Abuse in School, said legislators went to the same state-run libraries and begun ditching those weapons of mass abuse. After Dean took a break from campaign work near Logan University last year, IU was apparently further bribed and hoodwinked. In fact, according to Yitya Yaogwangowicz, communications director for the IU Police Department, \"Transgender teenage prostitutes were warning IU police to\n",
            "================================================================================\n",
            "Model prompt >>> Weenit is a good teacher\n",
            "======================================== SAMPLE 1 ========================================\n",
            "; a good financial adviser; a good partner\"; one of his finest, is also worth guarding. If he sides with any crowd, always armed—more opponents under their feet than over them—he must always at the first terms in enemy territory say, that the enemy are attacking, and be able to react to the new opening, expect that the old structures will be taken without the enemy being able to land a decisive blow. If Arakovsky writes that he is not a tyrant at the beginning of battle, he is certainly treating us as he speaks of us, and his chief adversary is old (now looked upon as old; as once and for all a pre-eminently military) from within:\n",
            "\n",
            "We have in this war situation, however, the same necessity of introducing a kind of pre-eminence that has hitherto been obliterated. It has increased the suffering of many avowed tyrants; it has weakened the spirit of the people. But we are not able to adequately take the steps under which we are able to organize the situation, and we have greatly increased the difficulty of doing so. And this has heightened the pressure the Allies have put upon us to bend and bend, but we cannot do this without the aid of anti-Fascist maneuver. These are the sort of maneuvers that are not sufficient to open an exchange of blows. But after a short time we take advantage of this. By uniting by martial methods we shall thereby gain contacts with the allies armed with weapons of terror. Indeed, we ought always to expect the results of this important step, which may for some years render us ready to exchange blows.\n",
            "\n",
            "Even Michael Scott's drive AGAINST THE DAIR defines a partial nature of the conduct by Yarl's Wood (1568) against our opponents. While virtually heralding the key brutal stabilg of the Palmerite Manifesto, we the North hastily regard it, have aided by Dallas spiritation, were perhaps these objectives the main object to which he moved our main lines. Years till now, during the sweeping Polish offensive upon Czarist Russia, our enemy had united to fight our Perichun by means of terror as though the enemy had some promise of revenge. In previous wars against the Pskovs we had divided 1,000 of our staffs into three groups, prompted by the feebleness of news stories, and capable of disregarding our orders. No one knew to what extent what several of our war officers know about our orders in our own minds, in the best\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5652, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 73, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 91, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 88, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1618, in __exit__\n",
            "    close_thread.start()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 851, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 551, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 295, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SUsFqpqi1Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}